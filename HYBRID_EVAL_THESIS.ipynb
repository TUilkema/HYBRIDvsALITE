{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3cffbcf-9b40-44dd-8319-f9a91352e456",
   "metadata": {},
   "source": [
    "# Hybrid template based schema mapping pipeline\n",
    "\n",
    "This notebook standardizes tenancy schedule data using a hybrid schema- and instance-based scoring model. It matches input CSV columns to a target schema(in YAML format) using both:\n",
    "\n",
    "- Schema-level metrics (meta-data related: header names, synonyms, token overlap)\n",
    "- Instance-level metrics (field values related: data types, numeric distributions, dates)\n",
    "\n",
    "Steps:\n",
    "- Configuration / some utility functions\n",
    "- Load schema\n",
    "- Define scoring metrics (seperated into schema and instance based)\n",
    "- Map raw columns to schema\n",
    "- Evaluate performance of the mapping against a ground truth file\n",
    "- Run steps of pipeline all at once\n",
    "- \n",
    "This is desgined to process multiple CSV files of tenancy schedules of different formats and match the columns to a target attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aae0af-436b-4604-b9c2-cf8d89d02f90",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "General config paths/parameters and all imports required\n",
    "\n",
    "The metric for matching column to attribute are seperated into schema and instance based\n",
    "The alpha parm is the blend value between the importance of schema vs instance based in the formula:\n",
    "    alpha * s_schema + (1 - alpha) * s_inst\n",
    "Threshold is the minimal required confidence to map a column to a attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b8181085-d222-4a82-951f-ba6aa10f1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "from pandas import Series, DataFrame\n",
    "from rapidfuzz import fuzz\n",
    "from rapidfuzz.distance import Levenshtein as Lev\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "CFG = {\n",
    "    \"schema_path\":  Path(r\"C:\\Users\\tim\\Documents\\THESIS\\Data\\target_schema_complete_repaired - full.yaml\"),\n",
    "    \"src_dir\":      Path(r\"C:/Users/tim/Documents/THESIS/Data/SCHEDULES\"),\n",
    "    \"out_dir\":      Path(r\"C:/Users/tim/Documents/THESIS/Data/processed\"),\n",
    "    \"ground_truth\": Path(r\"C:/Users/tim/Documents/THESIS/Data/GROUND_TRUTH - mapping.csv\"),\n",
    "\n",
    "    \"grid_size\": 4,\n",
    "    \"alpha\":     0.5,\n",
    "    \"threshold\": 0.5,\n",
    "    \"schema_w\": {\n",
    "        \"schema_synonym\":     1.0,\n",
    "        \"schema_jaccard\":     1.0,\n",
    "        \"schema_levenshtein\": 1.0,\n",
    "    },\n",
    "\n",
    "    \"instance_w\": {\n",
    "        \"instance_ks\":   1.0,\n",
    "        \"instance_type\": 1.0,\n",
    "        \"instance_dist\": 1.0,\n",
    "        \"instance_date\": 1.0,\n",
    "    }\n",
    "}\n",
    "\n",
    "CFG[\"out_dir\"].mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2ca62-2ae9-4d30-9a86-918421c12bd5",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "Basic function to help with the input data.\n",
    "Clean normalizes text and normalizes unicode.\n",
    "Extract_numeric function will extract the numeric values from more messy strings such as :'€ 249.492'or '3rd, 4th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e017f9c6-ae0b-4444-87d6-8f0e4ad93cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean  = lambda t: re.sub(r\"[^\\w\\s]\", \" \", unicodedata.normalize(\"NFKC\", str(t).lower())).strip()\n",
    "DATE_RX = re.compile(r\"\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})\\b\")\n",
    "NUM_RX  = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "\n",
    "def extract_numeric(values: pd.Series):\n",
    "    s = (values.dropna()\n",
    "                .str.replace('[€]', '', regex=True)\n",
    "                .str.replace('.', '', regex=False)\n",
    "                .str.replace(',', '.', regex=False))\n",
    "    return pd.to_numeric(s, errors='coerce').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6789e5-d1e3-4811-95d1-552ad5dda841",
   "metadata": {},
   "source": [
    "The YAML file is is based on the Data Requirements Dccument of CBRE and represents our target schema.\n",
    "This file contains all target attributes and information such as: Data types / synonyms / IQR / mean (when available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76654076-0c78-47b3-8c41-47358a546314",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SCHEMA: List[dict] = yaml.safe_load(CFG[\"schema_path\"].read_text())[\"attributes\"]\n",
    "for a in TARGET_SCHEMA:\n",
    "    a[\"name_clean\"] = clean(a[\"name\"])\n",
    "    a[\"syn_clean\"]  = [clean(s) for s in a.get(\"synonyms\", [])]\n",
    "\n",
    "TARGET_IDS = [a[\"id\"] for a in TARGET_SCHEMA]\n",
    "ATTR = {a[\"id\"]: a for a in TARGET_SCHEMA}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b3256-036e-42a8-9815-85ec77cc4bf8",
   "metadata": {},
   "source": [
    "## Schema based metrics\n",
    "\n",
    "These metrics are used in determining similarity between column and target attribute based on meta-data information such as name of column and synonyms.\n",
    "Synonym match is a fuzzy token similarity between synonyms and coumn name (takes the best synonym match).\n",
    "Jaccard overlap of tokens is used and normalized levenshtein distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8803f2ed-b2c7-4275-ac17-982540d69723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_synonym(h: str, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    txt = clean(h)\n",
    "    choices = [a[\"name_clean\"]] + a[\"syn_clean\"]\n",
    "    return max(fuzz.token_set_ratio(txt, c) for c in choices) / 100\n",
    "\n",
    "\n",
    "def schema_jaccard(h: str, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    s1 = set(clean(h).split())\n",
    "    best = 0.0\n",
    "    for s in [a[\"name_clean\"]] + a[\"syn_clean\"]:\n",
    "        s2 = set(s.split())\n",
    "        if s1 or s2:\n",
    "            best = max(best, len(s1 & s2) / len(s1 | s2))\n",
    "    return best\n",
    "\n",
    "\n",
    "def schema_levenshtein(h: str, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    txt = clean(h)\n",
    "    return max(Lev.normalized_similarity(txt, n) for n in [a[\"name_clean\"]] + a[\"syn_clean\"])\n",
    "\n",
    "\n",
    "SCHEMA_METRICS = {\n",
    "    \"schema_synonym\":     schema_synonym,\n",
    "    \"schema_jaccard\":     schema_jaccard,\n",
    "    \"schema_levenshtein\": schema_levenshtein,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2ec64-5c54-49d9-a1ad-1fde68338130",
   "metadata": {},
   "source": [
    "## Instance based metrics\n",
    "\n",
    "These functions/metric compare the actual values in the column fields to expectation of the target schema attribute.\n",
    "\n",
    "instance_type checks if the columns look like a numeric column total_rent columns will mostly contain numbers and some possible currency character. Floor columns can be filled with '4rd , 5th' as a cell value and thus be partially numeric filled.\n",
    "\n",
    "instance_date does the same thing but tries to determine if a column is a date value. We regex compare the cells to date formats.\n",
    "\n",
    "instance_ks compares the numeric distribution using the KS test.\n",
    "\n",
    "instance_dist compares the mean and IQR of the column against the target attribute values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c5a38a8-73b2-4b01-ba68-74c1327cfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_type(_, col: Series, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    if col.dropna().empty:\n",
    "        return 0.0\n",
    "    has_digit    = col.dropna().astype(str).str.contains(r\"\\d\").mean()\n",
    "    starts_digit = col.dropna().astype(str).str.match(r\"^\\s*\\d\").mean()\n",
    "    numeric_like = 0.7 * has_digit + 0.3 * starts_digit\n",
    "    return numeric_like if a[\"data_type\"] == \"decimal\" else 1 - numeric_like\n",
    "\n",
    "\n",
    "def instance_date(_, col: Series, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    if a[\"data_type\"] != \"date\" or col.dropna().empty:\n",
    "        return 0.0\n",
    "    parsed = col.dropna().astype(str).apply(lambda v: bool(DATE_RX.search(v))).mean()\n",
    "    return round(parsed, 4)\n",
    "\n",
    "\n",
    "def instance_ks(_, col: Series, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    if a[\"data_type\"] != \"decimal\":\n",
    "        return None\n",
    "    x = extract_numeric(col)\n",
    "    if len(x) < 15:\n",
    "        return None\n",
    "    if \"ref_sample\" in a:\n",
    "        ref = pd.Series(a[\"ref_sample\"], dtype=float)\n",
    "    elif {\"mean\", \"iqr\"} <= a.keys():\n",
    "        mu = a[\"mean\"]\n",
    "        sigma = (a[\"iqr\"][\"q3\"] - a[\"iqr\"][\"q1\"]) / 1.349\n",
    "        rng = np.random.default_rng(0)\n",
    "        ref = pd.Series(rng.normal(mu, sigma, len(x)))\n",
    "    else:\n",
    "        return None\n",
    "    stat, _ = ks_2samp(x, ref)\n",
    "    return round(1 - stat, 4)\n",
    "\n",
    "\n",
    "def instance_dist(_, col: Series, aid: str):\n",
    "    a = ATTR[aid]\n",
    "    if a[\"data_type\"] != \"decimal\" or {\"mean\", \"iqr\"} - a.keys():\n",
    "        return None\n",
    "    x = extract_numeric(col)\n",
    "    if x.empty:\n",
    "        return None\n",
    "    mu_t, q1, q3 = a[\"mean\"], a[\"iqr\"][\"q1\"], a[\"iqr\"][\"q3\"]\n",
    "    iqr = q3 - q1 if q3 != q1 else 1e-6\n",
    "    dist = abs(x.mean() - mu_t) / iqr\n",
    "    in_iqr = x.between(q1, q3).mean()\n",
    "    return round(0.5 * max(0, 1 - dist) + 0.5 * in_iqr, 4)\n",
    "\n",
    "\n",
    "INSTANCE_METRICS = {\n",
    "    \"instance_type\": instance_type,\n",
    "    \"instance_date\": instance_date,\n",
    "    \"instance_ks\":   instance_ks,\n",
    "    \"instance_dist\": instance_dist\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e31f1-8cd9-4713-8522-c10483998289",
   "metadata": {},
   "source": [
    "## Hybrid scoring functino\n",
    "\n",
    "The function which combines the schema based and instance based scores using a weighted average.\n",
    "The formula is the same as given in the thesis: alpha parameter controls overall balance between schema/instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "519850fd-ac87-4260-8302-96f8d720b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted(metrics, weights, args):\n",
    "    score = 0.0\n",
    "    total_weight = 0.0\n",
    "\n",
    "    for name, func in metrics.items():\n",
    "        weight = weights.get(name, 0.0)\n",
    "        if weight == 0:\n",
    "            continue\n",
    "        value = func(*args)\n",
    "        if value is None:\n",
    "            continue\n",
    "        score += weight * value\n",
    "        total_weight += weight\n",
    "\n",
    "    return score / total_weight if total_weight else 0.0\n",
    "\n",
    "def hybrid_score(header, col, aid, weight_tbl=None):\n",
    "    if weight_tbl is None:\n",
    "        schema_w = CFG[\"schema_w\"]\n",
    "        instance_w = CFG[\"instance_w\"]\n",
    "    else:\n",
    "        schema_w = weight_tbl[aid]\n",
    "        instance_w = weight_tbl[aid]\n",
    "\n",
    "    schema_score = weighted(SCHEMA_METRICS, schema_w, (header, aid))\n",
    "    instance_score = weighted(INSTANCE_METRICS, instance_w, (None, col, aid))\n",
    "\n",
    "    alpha = CFG[\"alpha\"]\n",
    "    return alpha * schema_score + (1 - alpha) * instance_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf985f5-7b22-48a7-beab-ed5652af4b24",
   "metadata": {},
   "source": [
    "## Column to target attribute mapping\n",
    "\n",
    "\n",
    "For every column in raw tenancy schedule we (potentially) map it to a target attribute by:\n",
    "- Compute score for all metrics to all target attriobutes.\n",
    "- Select best scoring attribte\n",
    "- Use the hungarian algorithmm to enforce global optimal mapping and one-to-one mapping (Total rent could have very high scores in mapping to total_rent and total_area but we only want to map it to that which gives us global optimal score).\n",
    "- Rename and reorder columns according to target schema\n",
    "\n",
    "The result/return is the standardized tenancy schedule with a DataFrame and mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d882c9f1-1887-4cf8-ae71-5950035d8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dataframe(df_raw, src_name, weight_tbl=None):\n",
    "    raw_cols = list(df_raw.columns)\n",
    "    score_mat = np.empty((len(raw_cols), len(TARGET_IDS)), dtype=float)\n",
    "\n",
    "    for i, raw in enumerate(raw_cols):\n",
    "        for j, aid in enumerate(TARGET_IDS):\n",
    "            score_mat[i, j] = hybrid_score(raw, df_raw[raw], aid, weight_tbl)\n",
    "\n",
    "    row_idx, col_idx = linear_sum_assignment(-score_mat)\n",
    "\n",
    "    mapping = {\n",
    "        raw_cols[i]: (TARGET_IDS[j], score_mat[i, j])\n",
    "        for i, j in zip(row_idx, col_idx)\n",
    "        if score_mat[i, j] >= CFG[\"threshold\"]\n",
    "    }\n",
    "\n",
    "    dbg_df = pd.DataFrame([\n",
    "        {\n",
    "            \"raw_column\": raw,\n",
    "            \"target_attr\": attr,\n",
    "            \"score\": score,\n",
    "            \"accepted\": score >= CFG[\"threshold\"]\n",
    "        }\n",
    "        for raw, (attr, score) in mapping.items()\n",
    "    ])\n",
    "\n",
    "    std_df = df_raw.rename(columns={r: aid for r, (aid, _) in mapping.items()})\n",
    "    std_df = std_df.reindex(columns=TARGET_IDS)\n",
    "    std_df[\"__src__\"] = src_name\n",
    "\n",
    "    if not CFG.get(\"opt_mode\", False):\n",
    "        out_path = CFG[\"out_dir\"] / f\"{Path(src_name).stem}_debug.xlsx\"\n",
    "        dbg_df.to_excel(out_path, index=False)\n",
    "\n",
    "    return std_df, dbg_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48640e05-a486-4a97-ae45-1c9c30b62d65",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "\n",
    "Compare predicted mapping/standardised schedule to manually created ground truth using true positives (TP), false positives (FP), false negatives (FN). With this we calculate precision/recall and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e6512aa-bb97-4d92-a774-7c4889528688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(out_dir):\n",
    "    rows = []\n",
    "\n",
    "    for file in out_dir.glob(\"*_debug.xlsx\"):\n",
    "        try:\n",
    "            dbg = pd.read_excel(file)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if not {\"raw_column\", \"target_attr\"}.issubset(dbg.columns):\n",
    "            continue\n",
    "\n",
    "        accepted = dbg if \"accepted\" not in dbg.columns else dbg[dbg[\"accepted\"]]\n",
    "        fmt = re.split(r\"[_\\-]\", file.stem.replace(\"_debug\", \"\"))[0].upper()\n",
    "\n",
    "        for _, row in accepted.iterrows():\n",
    "            rows.append({\n",
    "                \"FORMAT\": fmt,\n",
    "                \"RAW_HEADER\": row[\"raw_column\"],\n",
    "                \"PREDICTED_ATTR\": row[\"target_attr\"],\n",
    "                \"FILE_NAME\": file.name.replace(\"_debug.xlsx\", \".csv\")\n",
    "            })\n",
    "\n",
    "    if rows:\n",
    "        pred = pd.DataFrame(rows)\n",
    "        pred[\"RAW_HEADER_CLEAN\"] = pred[\"RAW_HEADER\"].apply(clean)\n",
    "    else:\n",
    "        pred = pd.DataFrame(columns=[\n",
    "            \"FORMAT\", \"RAW_HEADER\", \"PREDICTED_ATTR\", \"FILE_NAME\", \"RAW_HEADER_CLEAN\"\n",
    "        ])\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7310f891-bb01-456c-8791-bc25d5b10487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(verbose=True):\n",
    "    gt = pd.read_csv(CFG[\"ground_truth\"])\n",
    "    gt = gt[gt[\"TARGET_ATTRIBUTE\"] != \"omit\"].copy()\n",
    "    gt[\"RAW_HEADER_CLEAN\"] = gt[\"RAW_HEADER\"].apply(clean)\n",
    "\n",
    "    pred = collect_predictions(CFG[\"out_dir\"])\n",
    "\n",
    "    merged = pred.merge(\n",
    "        gt[[\"FORMAT\", \"RAW_HEADER_CLEAN\", \"TARGET_ATTRIBUTE\"]],\n",
    "        on=[\"FORMAT\", \"RAW_HEADER_CLEAN\"],\n",
    "        how=\"outer\",\n",
    "        indicator=True,\n",
    "    )\n",
    "\n",
    "    def label(row):\n",
    "        if row[\"_merge\"] == \"both\":\n",
    "            return \"TP\" if row[\"PREDICTED_ATTR\"] == row[\"TARGET_ATTRIBUTE\"] else \"FP\"\n",
    "        return \"FP\" if row[\"_merge\"] == \"left_only\" else \"FN\"\n",
    "\n",
    "    merged[\"STATUS\"] = merged.apply(label, axis=1)\n",
    "\n",
    "    tp = (merged[\"STATUS\"] == \"TP\").sum()\n",
    "    fp = (merged[\"STATUS\"] == \"FP\").sum()\n",
    "    fn = (merged[\"STATUS\"] == \"FN\").sum()\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp else 0.0\n",
    "    recall    = tp / (tp + fn) if tp + fn else 0.0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True positives : {tp}\")\n",
    "        print(f\"False positives: {fp}\")\n",
    "        print(f\"False negatives: {fn}\")\n",
    "        print(f\"Precision      : {precision}\")\n",
    "        print(f\"Recall         : {recall}\")\n",
    "        print(f\"F1 score       : {f1}\")\n",
    "\n",
    "        merged.to_csv(CFG[\"out_dir\"] / \"prediction_vs_truth.csv\", index=False)\n",
    "\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4362a6-b082-4c28-b593-0bd061b5bfd2",
   "metadata": {},
   "source": [
    "## Run pipeline\n",
    "\n",
    "This block processes all CSV files in input folder, applies column mapping logic and outputs the standardised schedule. Finally we evaluate te results using the evaluation block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "795cd170-f1ab-4abd-a4ca-ca9fd977816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null percentage : 47.84313725490196%\n",
      "True positives : 56\n",
      "False positives: 15\n",
      "False negatives: 17\n",
      "Precision      : 0.7887323943661971\n",
      "Recall         : 0.7671232876712328\n",
      "F1 score       : 0.7777777777777778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    std_tables: List[DataFrame] = []\n",
    "    for csv in sorted(CFG[\"src_dir\"].glob(\"*.csv\")):\n",
    "        raw_df = pd.read_csv(csv, dtype=str)\n",
    "        std_df, _ = map_dataframe(raw_df, csv.name)\n",
    "        std_tables.append(std_df)\n",
    "\n",
    "    master = pd.concat(std_tables, ignore_index=True)\n",
    "    master.to_excel(CFG[\"out_dir\"] / \"standardised_schedules.xlsx\", index=False)\n",
    "    \n",
    "    total_cells = master.shape[0] * master.shape[1]\n",
    "    num_nulls = master.isna().sum().sum()\n",
    "    null_pct = 100 * num_nulls / total_cells if total_cells else 0\n",
    "    print(f\"Null percentage : {null_pct:}%\")\n",
    "\n",
    "    evaluate()\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6454c258-abcd-4228-9a22-7144e16d21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_once(alpha, threshold, schema_w, instance_w):\n",
    "    CFG[\"alpha\"]      = alpha\n",
    "    CFG[\"threshold\"]  = threshold\n",
    "    CFG[\"schema_w\"]   = schema_w\n",
    "    CFG[\"instance_w\"] = instance_w\n",
    "\n",
    "    for f in CFG[\"out_dir\"].glob(\"*_debug.xlsx\"):\n",
    "        f.unlink(missing_ok=True)\n",
    "\n",
    "    for csv in CFG[\"src_dir\"].glob(\"*.csv\"):\n",
    "        df_raw = pd.read_csv(csv, dtype=str)\n",
    "        std_df, _ = map_dataframe(df_raw, csv.name)\n",
    "\n",
    "    W = {\n",
    "        aid: {**schema_w, **instance_w}\n",
    "        for aid in TGT_IDS\n",
    "    }\n",
    "    f1 = evaluate(verbose=False)\n",
    "\n",
    "    print(f\"α={alpha} θ={threshold} F1={f1}\")\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd532c24-08ab-4d94-853b-4a4d53eba56f",
   "metadata": {},
   "source": [
    "## Optimize alpha and threshold\n",
    "\n",
    "We try different combinations of Alpha and Threshold to get the best F1 score.\n",
    "We use this combination later on as well itertools is used to iterate over all combinations of alpha/threshold and the best combination is outputted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5216a872-45ce-43d5-8e81-b0e50e278929",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_W = CFG[\"schema_w\"]\n",
    "INSTANCE_W = CFG[\"instance_w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c859bdd-7e50-44c0-9542-b8432ca79ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alphas = [0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]\n",
    "#thresholds = [0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75]\n",
    "\n",
    "alphas = np.linspace(0.35, 0.75, num=int((0.75 - 0.35) / 0.02) + 1).round(2).tolist()\n",
    "thresholds = np.linspace(0.35, 0.75, num=int((0.75 - 0.35) / 0.02) + 1).round(2).tolist()\n",
    "\n",
    "TGT_IDS = list(ATTR.keys()) \n",
    "best = {\"f1\": -1}\n",
    "\n",
    "for α, θ in itertools.product(alphas, thresholds):\n",
    "    f1 = run_once(α, θ, deepcopy(SCHEMA_W), deepcopy(INSTANCE_W))\n",
    "    if f1 > best[\"f1\"]:\n",
    "        best = dict(alpha=α, threshold=θ, schema_w=SCHEMA_W, instance_w=INSTANCE_W, f1=f1)\n",
    "\n",
    "print(\"Best config:\", best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d173328-80c6-4833-9942-20e6f0e01dcb",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Alpha = 0.5 chosen and threshold = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "28ff7a6e-157b-4174-9e2f-ba6092a02eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG[\"alpha\"] = 0.5\n",
    "CFG[\"threshold\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a584f08-6cfa-49f5-8c78-2efd2a3fb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv(CFG[\"ground_truth\"])\n",
    "gt = gt[gt[\"TARGET_ATTRIBUTE\"] != \"omit\"]\n",
    "gt[\"RAW_HEADER_CLEAN\"] = gt[\"RAW_HEADER\"].apply(clean)\n",
    "FAST_GT = gt[[\"FORMAT\", \"RAW_HEADER_CLEAN\", \"TARGET_ATTRIBUTE\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "02e41e19-8bfe-4659-817c-2b9af36b5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_f1(weight_tbl):\n",
    "    rows = []\n",
    "\n",
    "    for csv in sorted(CFG[\"src_dir\"].glob(\"*.csv\")):\n",
    "        df_raw = pd.read_csv(csv, dtype=str)\n",
    "        fmt = re.split(r\"[_\\-]\", csv.stem)[0].upper()\n",
    "\n",
    "        mapping = resolve_mapping_tensor(\n",
    "            df_raw,\n",
    "            csv,  # using csv as the file_key\n",
    "            weights=weight_tbl,\n",
    "            alpha=CFG[\"alpha\"],\n",
    "            thresh=CFG[\"threshold\"],\n",
    "        )\n",
    "\n",
    "        for raw, aid in mapping.items():\n",
    "            rows.append({\n",
    "                \"FORMAT\": fmt,\n",
    "                \"RAW_HEADER_CLEAN\": raw,\n",
    "                \"PREDICTED_ATTR\": aid,\n",
    "                \"FILE_NAME\": csv.name,\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    pred = pd.DataFrame(rows)\n",
    "    gt = FAST_GT\n",
    "\n",
    "    merged = pred.merge(gt, on=[\"FORMAT\", \"RAW_HEADER_CLEAN\"], how=\"outer\", indicator=True)\n",
    "\n",
    "    tp = ((merged[\"_merge\"] == \"both\") & (merged[\"PREDICTED_ATTR\"] == merged[\"TARGET_ATTRIBUTE\"])).sum()\n",
    "    fp = ((merged[\"_merge\"] == \"both\") & (merged[\"PREDICTED_ATTR\"] != merged[\"TARGET_ATTRIBUTE\"])).sum()\n",
    "    fp += (merged[\"_merge\"] == \"left_only\").sum()\n",
    "    fn = (merged[\"_merge\"] == \"right_only\").sum()\n",
    "\n",
    "    p = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    r = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) else 0.0\n",
    "\n",
    "    return p, r, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec769f69-f1d5-4996-ab68-c330977f8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = np.linspace(0, 1, CFG[\"grid_size\"])\n",
    "METRICS = list(SCHEMA_METRICS) + list(INSTANCE_METRICS)\n",
    "\n",
    "WEIGHTS = {aid: {m: 0.5 for m in METRICS} for aid in TGT_IDS}\n",
    "\n",
    "# run F1 evaluation for one specific weight combination\n",
    "def f1_for_combo(aid, trial_vec):\n",
    "    trial_weights = dict(zip(METRICS, trial_vec))\n",
    "    temp_weights = deepcopy(WEIGHTS)\n",
    "    temp_weights[aid].update(trial_weights)\n",
    "    return fast_f1(temp_weights), trial_weights\n",
    "\n",
    "# try combinations of weights for a single attribute\n",
    "def search_attribute(aid):\n",
    "    all_combos = list(itertools.product(GRID, repeat=len(METRICS)))\n",
    "    print(f\"Trying {len(all_combos)} combinations for {aid}...\")\n",
    "    results = Parallel(n_jobs=mp.cpu_count() - 1, backend=\"loky\", verbose=5)(\n",
    "        delayed(f1_for_combo)(aid, vec) for vec in all_combos\n",
    "    )\n",
    "    \n",
    "    # Finding best set\n",
    "    best_f1 = -1.0\n",
    "    best_weights = {}\n",
    "\n",
    "    for score, trial_weights in results:\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_weights = trial_weights\n",
    "\n",
    "    WEIGHTS[aid].update(best_weights)\n",
    "    print(f\"Best F1 for {aid}: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "79cc56d8-7187-4fdb-ad62-dbe53191242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_all_attributes(num_passes=1):\n",
    "    grid = np.linspace(0.1, 1.0, CFG[\"grid_size\"])\n",
    "    metrics = list(SCHEMA_METRICS) + list(INSTANCE_METRICS)\n",
    "    all_combos = list(itertools.product(grid, repeat=len(metrics)))\n",
    "\n",
    "    for epoch in range(num_passes):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        for aid in TGT_IDS:\n",
    "            print(f\"Searching weights for: {aid}\")\n",
    "            prev_weights = deepcopy(WEIGHTS[aid])\n",
    "            results = Parallel(n_jobs=mp.cpu_count() - 1, backend=\"loky\", verbose=10)(delayed(f1_for_combo)(aid, vec) for vec in all_combos)\n",
    "            best_f1 = -1.0\n",
    "            best_weights = None\n",
    "            for (p, r, f1), trial_weights in results:\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_weights = trial_weights\n",
    "\n",
    "            WEIGHTS[aid].update(best_weights)\n",
    "            print(f\"Best F1: {best_f1} (Precision: {p}, Recall: {r})\")\n",
    "\n",
    "            if best_weights != prev_weights:\n",
    "                print(f\" Weights updated for {aid}\")\n",
    "\n",
    "    out = Path(\"attr_weights.json\")\n",
    "    out.write_text(json.dumps(WEIGHTS, indent=2))\n",
    "    print(f\"Saved attribute weights to {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f1aa51c7-e4d3-408a-a66d-82ecd3fc32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mapping_for_all_sources(weights):\n",
    "    results = []\n",
    "\n",
    "    for csv_file in sorted(CFG[\"src_dir\"].glob(\"*.csv\")):\n",
    "        df = pd.read_csv(csv_file, dtype=str)\n",
    "        std_df, _ = map_dataframe(df, csv_file.name, weights)\n",
    "        results.append(std_df)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0ae6a659-8611-4ef8-8e9e-a75c24cfde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_mapping_tensor(df_raw, file_key, weights, alpha, thresh):\n",
    "    schema = SCHEMA_T[file_key]\n",
    "    instance = INSTANCE_T[file_key]\n",
    "    headers = HEADER_CLEAN[file_key]\n",
    "\n",
    "    num_raw, num_attr = schema.shape[:2]\n",
    "    score_matrix = np.full((num_raw, num_attr), -1e9)\n",
    "\n",
    "    for j, attr_id in enumerate(TGT_IDS):\n",
    "        schema_w = np.array([weights[attr_id][name] for name in SCHEMA_METRICS])\n",
    "        instance_w = np.array([weights[attr_id][name] for name in INSTANCE_METRICS])\n",
    "\n",
    "        schema_score = (schema[:, j, :] * schema_w).sum(axis=1) / schema_w.sum()\n",
    "        instance_score = (instance[:, j, :] * instance_w).sum(axis=1) / instance_w.sum()\n",
    "\n",
    "        score_matrix[:, j] = alpha * schema_score + (1 - alpha) * instance_score\n",
    "\n",
    "    row_idx, col_idx = linear_sum_assignment(-score_matrix)\n",
    "\n",
    "    return {\n",
    "        headers[i]: TGT_IDS[j]\n",
    "        for i, j in zip(row_idx, col_idx)\n",
    "        if score_matrix[i, j] >= thresh\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "814d33b0-fefd-4e39-87e9-5d31e163ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached similarity helpers\n",
    "@lru_cache(maxsize=None)\n",
    "def token_ratio(a, b):\n",
    "    return fuzz.token_set_ratio(a, b)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def lev_similarity(a, b):\n",
    "    return Lev.normalized_similarity(a, b)\n",
    "\n",
    "# Schema-based similarity metrics\n",
    "def schema_synonym(header, attr_id):\n",
    "    attr = ATTR[attr_id]\n",
    "    h_clean = clean(header)\n",
    "    options = [attr[\"name_clean\"]] + attr[\"syn_clean\"]\n",
    "    return max(token_ratio(h_clean, s) for s in options) / 100\n",
    "\n",
    "def schema_levenshtein(header, attr_id):\n",
    "    attr = ATTR[attr_id]\n",
    "    h_clean = clean(header)\n",
    "    options = [attr[\"name_clean\"]] + attr[\"syn_clean\"]\n",
    "    return max(lev_similarity(h_clean, s) for s in options)\n",
    "\n",
    "# Jaccard stays as-is but memoized\n",
    "schema_jaccard = lru_cache(maxsize=None)(schema_jaccard)\n",
    "\n",
    "# Load CSVs\n",
    "RAW_DFS = {\n",
    "    path: pd.read_csv(path, dtype=str)\n",
    "    for path in CFG[\"src_dir\"].glob(\"*.csv\")\n",
    "}\n",
    "\n",
    "# Storage for tensors and headers\n",
    "SCHEMA_T = {}\n",
    "INSTANCE_T = {}\n",
    "HEADER_CLEAN = {}\n",
    "\n",
    "# Get list of metric functions\n",
    "schema_metrics = list(SCHEMA_METRICS.values())\n",
    "instance_metrics = list(INSTANCE_METRICS.values())\n",
    "\n",
    "# Compute schema + instance metric tensors for each file\n",
    "for path, df in RAW_DFS.items():\n",
    "    cleaned_headers = [clean(h) for h in df.columns]\n",
    "    HEADER_CLEAN[path] = cleaned_headers\n",
    "\n",
    "    num_headers = len(cleaned_headers)\n",
    "    num_attrs = len(TARGET_IDS)\n",
    "\n",
    "    schema_tensor = np.zeros((num_headers, num_attrs, len(schema_metrics)), dtype=np.float32)\n",
    "    instance_tensor = np.zeros((num_headers, num_attrs, len(instance_metrics)), dtype=np.float32)\n",
    "\n",
    "    for j, attr_id in enumerate(TARGET_IDS):\n",
    "        for i, h in enumerate(cleaned_headers):\n",
    "            for k, func in enumerate(schema_metrics):\n",
    "                schema_tensor[i, j, k] = func(h, attr_id)\n",
    "\n",
    "        for i, raw_col in enumerate(df.columns):\n",
    "            col_data = df[raw_col]\n",
    "            for k, func in enumerate(instance_metrics):\n",
    "                val = func(None, col_data, attr_id)\n",
    "                instance_tensor[i, j, k] = 0.0 if val is None else val\n",
    "\n",
    "    SCHEMA_T[path] = schema_tensor\n",
    "    INSTANCE_T[path] = instance_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "311e05a3-f068-4f4f-926a-0079dd170b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Searching weights for: property_id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=31)]: Using backend LokyBackend with 31 concurrent workers.\n",
      "[Parallel(n_jobs=31)]: Done  10 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=31)]: Done  23 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=31)]: Done  36 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=31)]: Done  51 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=31)]: Done  66 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=31)]: Done  83 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=31)]: Done 100 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=31)]: Done 119 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=31)]: Done 138 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=31)]: Done 159 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=31)]: Done 180 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=31)]: Done 203 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=31)]: Done 226 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=31)]: Done 251 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=31)]: Done 276 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=31)]: Done 303 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=31)]: Done 330 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=31)]: Done 359 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=31)]: Done 388 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=31)]: Done 419 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=31)]: Done 450 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=31)]: Done 483 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=31)]: Done 516 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=31)]: Done 551 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=31)]: Done 586 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=31)]: Done 623 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=31)]: Done 660 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=31)]: Done 699 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=31)]: Done 738 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=31)]: Done 779 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=31)]: Done 820 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=31)]: Done 863 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=31)]: Done 906 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=31)]: Done 951 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=31)]: Done 996 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=31)]: Done 1043 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=31)]: Done 1090 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=31)]: Done 1139 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=31)]: Done 1188 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=31)]: Done 1239 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=31)]: Done 1290 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=31)]: Done 1343 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=31)]: Done 1396 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=31)]: Done 1451 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=31)]: Done 1506 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=31)]: Done 1563 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=31)]: Done 1620 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=31)]: Done 1679 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=31)]: Done 1738 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=31)]: Done 1799 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=31)]: Done 1860 tasks      | elapsed:   14.3s\n",
      "[Parallel(n_jobs=31)]: Done 1923 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=31)]: Done 1986 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=31)]: Done 2051 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=31)]: Done 2116 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=31)]: Done 2183 tasks      | elapsed:   15.5s\n",
      "[Parallel(n_jobs=31)]: Done 2250 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=31)]: Done 2319 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=31)]: Done 2388 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=31)]: Done 2459 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=31)]: Done 2530 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=31)]: Done 2603 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=31)]: Done 2676 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=31)]: Done 2751 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=31)]: Done 2826 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=31)]: Done 2903 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=31)]: Done 2980 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=31)]: Done 3059 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=31)]: Done 3138 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=31)]: Done 3219 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=31)]: Done 3300 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=31)]: Done 3383 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=31)]: Done 3466 tasks      | elapsed:   20.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# we optimize weight and set the mode for printing purposes\u001b[39;00m\n\u001b[0;32m      9\u001b[0m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43moptimise_all_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Run mapping using learned weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[132], line 11\u001b[0m, in \u001b[0;36moptimise_all_attributes\u001b[1;34m(num_passes)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching weights for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m prev_weights \u001b[38;5;241m=\u001b[39m deepcopy(WEIGHTS[aid])\n\u001b[1;32m---> 11\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloky\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf1_for_combo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43maid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_combos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m best_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     13\u001b[0m best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FAST_GT = (\n",
    "    pd.read_csv(CFG[\"ground_truth\"])\n",
    "      .query(\"TARGET_ATTRIBUTE != 'omit'\")\n",
    "      .assign(RAW_HEADER_CLEAN=lambda df: df[\"RAW_HEADER\"].map(clean))\n",
    "      [[\"FORMAT\", \"RAW_HEADER_CLEAN\", \"TARGET_ATTRIBUTE\"]]\n",
    ")\n",
    "\n",
    "# we optimize weight and set the mode for printing purposes\n",
    "CFG[\"opt_mode\"] = True\n",
    "optimise_all_attributes()\n",
    "CFG[\"opt_mode\"] = False\n",
    "\n",
    "# Run mapping using learned weights\n",
    "standardized_tables = run_mapping_for_all_sources(WEIGHTS)\n",
    "final_df = pd.concat(standardized_tables, ignore_index=True)\n",
    "\n",
    "# Calculate and print null percentage\n",
    "total = final_df.shape[0] * len(TGT_IDS)\n",
    "nulls = final_df[TGT_IDS].isna().sum().sum()\n",
    "print(\"Final null %:\", 100 * nulls / total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5372c9a-d9ec-4f6e-a3a3-faf8a6cdc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calc avg weights after optimization per datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "00e3644b-124b-417a-acc1-6d32aaaec3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string\n",
      "  Avg schema weight  : 0.1333333333333334\n",
      "  Avg instance weight: 0.2625\n",
      "  Gap                : 0.12916666666666662\n",
      "decimal\n",
      "  Avg schema weight  : 0.11428571428571435\n",
      "  Avg instance weight: 0.3892857142857142\n",
      "  Gap                : 0.2749999999999998\n",
      "date\n",
      "  Avg schema weight  : 0.09999999999999999\n",
      "  Avg instance weight: 0.47500000000000003\n",
      "  Gap                : 0.37500000000000006\n",
      "\n",
      "Wilcoxon test result:\n",
      "  Statistic: 3.0\n",
      "  P-value  : 0.0004908453484823393\n"
     ]
    }
   ],
   "source": [
    "with open(\"attr_weights grid_4.json\") as f:\n",
    "    weights = json.load(f)\n",
    "\n",
    "with open(r\"C:\\Users\\tim\\Documents\\THESIS\\Data\\target_schema_complete_repaired - full.yaml\") as f:\n",
    "    schema = yaml.safe_load(f)\n",
    "\n",
    "schema_metrics = {\"schema_synonym\", \"schema_jaccard\", \"schema_levenshtein\"}\n",
    "instance_metrics = {\"instance_type\", \"instance_date\", \"instance_ks\", \"instance_dist\"}\n",
    "\n",
    "# Group weights by data type\n",
    "by_type = defaultdict(list)\n",
    "for attr in schema[\"attributes\"]:\n",
    "    aid = attr[\"id\"]\n",
    "    dtype = attr[\"data_type\"]\n",
    "    if aid in weights:\n",
    "        by_type[dtype].append(weights[aid])\n",
    "\n",
    "# Print average weights per type\n",
    "for dtype in [\"string\", \"decimal\", \"date\"]:\n",
    "    s_sum = i_sum = s_count = i_count = 0\n",
    "\n",
    "    for w in by_type[dtype]:\n",
    "        for metric, value in w.items():\n",
    "            if metric in schema_metrics:\n",
    "                s_sum += value\n",
    "                s_count += 1\n",
    "            elif metric in instance_metrics:\n",
    "                i_sum += value\n",
    "                i_count += 1\n",
    "\n",
    "    s_avg = s_sum / s_count if s_count else 0\n",
    "    i_avg = i_sum / i_count if i_count else 0\n",
    "    gap = i_avg - s_avg\n",
    "\n",
    "    print(f\"{dtype}\")\n",
    "    print(f\"  Avg schema weight  : {s_avg}\")\n",
    "    print(f\"  Avg instance weight: {i_avg}\")\n",
    "    print(f\"  Gap                : {gap}\")\n",
    "\n",
    "schema_avgs = []\n",
    "instance_avgs = []\n",
    "\n",
    "for aid, w in weights.items():\n",
    "    s_avg = sum(w.get(m, 0) for m in schema_metrics) / len(schema_metrics)\n",
    "    i_avg = sum(w.get(m, 0) for m in instance_metrics) / len(instance_metrics)\n",
    "    schema_avgs.append(s_avg)\n",
    "    instance_avgs.append(i_avg)\n",
    "\n",
    "stat, p = wilcoxon(instance_avgs, schema_avgs)\n",
    "print(\"Wilcoxon test result:\")\n",
    "print(f\"  Statistic: {stat}\")\n",
    "print(f\"  P-value  : {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab126d-ce8e-4b0b-82a9-72dd19eff631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc2a52-a13c-40bc-9fcc-50111482ae7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
