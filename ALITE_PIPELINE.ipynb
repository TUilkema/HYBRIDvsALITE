{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5ba09d2c-e381-46c3-8222-194d00849b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, itertools, json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Input is a folder filled with csv files of tenancy schedules\n",
    "\n",
    "CFG = {\n",
    "    \"src_dir\": Path(r\"C:\\Users\\tim\\Documents\\Thesis\\Programming\\Data\\SCHEDULES\"),    \n",
    "    \"out_dir\": Path(r\"C:\\Users\\tim\\Documents\\Thesis\\Programming\\Data\\processed\"),   \n",
    "}\n",
    "CFG[\"out_dir\"].mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0cc74-7d61-4b7d-b661-ddf52f98bc6f",
   "metadata": {},
   "source": [
    "## Utlity helpers\n",
    "We have 2 helper functions here:\n",
    "\n",
    "clean: performs ASCII-based normalisation on column names for easier matching.\n",
    "\n",
    "embed_column: converts a single dataframe column into a vector embedding as described in ALITE using SBERT. (They used a research model here called TURL)  \n",
    "\n",
    "These embeddings are the foundation for clustering semantically similar columns across different tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6eea2518-1658-4945-813f-6abda17abedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\s]\", \" \",unicodedata.normalize(\"NFKC\", str(text).lower())).strip()\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def embed_column(col: pd.Series, col_name: str, sample: int = 50) -> np.ndarray:\n",
    "    non_null = col.dropna().astype(str)\n",
    "    if non_null.empty:\n",
    "        return np.zeros(embedder.get_sentence_embedding_dimension())\n",
    "\n",
    "    samples = non_null.sample(min(sample, len(non_null)), random_state=0).tolist()\n",
    "    inputs = [f\"{col_name}: {val}\" for val in samples]\n",
    "    vecs = embedder.encode(inputs, show_progress_bar=False)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1fe4e-5e41-41fd-9fab-50844287c443",
   "metadata": {},
   "source": [
    "## Schema alignment through clustering columns\n",
    "\n",
    "This function assigns integration IDS to each column across all input tenancy schedule tables.\n",
    "\n",
    "STEPS:\n",
    "- Embed all columns using SBERT with the column name and value as context.\n",
    "- Compute cosine distance matrix.\n",
    "- Try to force that no columns from the same table are clustered by setting distances within table to 2.0\n",
    "- Hierarchical clustering is applied to group similar columns.\n",
    "- The number of clusters k is chosen using the silhouette score to maximise cluster quality/accuracy. The score indicates how well it merges and can be compared to see how well a column fit into other clusters.\n",
    "- Columns assigned to the same cluster are given the common integration ID (in final csv they are: iid_000 ,iid_001 etc).\n",
    "\n",
    "So in this part we try to group semantically similar columns without combining columns from the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e5bffd2-bd6a-4565-812c-ff2b2a81e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_iids(tables: Dict[str, pd.DataFrame]) -> Dict[str, Dict[str, str]]:\n",
    "    cols, vecs = [], []\n",
    "    for tname, df in tables.items():\n",
    "        for cname in df.columns:\n",
    "            v = embed_column(df[cname], cname)\n",
    "            if np.linalg.norm(v) > 1e-6:\n",
    "                cols.append((tname, cname))\n",
    "                vecs.append(v)\n",
    "\n",
    "    X = normalize(np.vstack(vecs))\n",
    "    D = pairwise_distances(X, metric=\"cosine\")\n",
    "\n",
    "    tbl = np.array([t for t, _ in cols])\n",
    "    D[tbl[:, None] == tbl[None, :]] = 2.0    \n",
    "\n",
    "    min_k = max(len(df.columns) for df in tables.values())\n",
    "    max_k = len(X) - 1\n",
    "    \n",
    "    if min_k == 1:\n",
    "        best_k, best_s = 1, -1\n",
    "    else:\n",
    "        best_k, best_s = min_k, -1\n",
    "        for k in range(min_k, max_k + 1):\n",
    "            labels = AgglomerativeClustering(n_clusters=k, metric=\"precomputed\", linkage=\"average\").fit_predict(D)\n",
    "            s = silhouette_score(X, labels, metric=\"cosine\")\n",
    "            if s > best_s:\n",
    "                best_k, best_s = k, s\n",
    "\n",
    "    labels = AgglomerativeClustering(n_clusters=best_k, metric=\"precomputed\", linkage=\"average\").fit_predict(D)\n",
    "    lab2iid = {lab: f\"iid_{i:03d}\" for i, lab in enumerate(np.unique(labels))}\n",
    "    mapping = {}\n",
    "    \n",
    "    for (t, c), lab in zip(cols, labels):\n",
    "        mapping.setdefault(t, {})[c] = lab2iid[lab]\n",
    "        \n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c2203-3929-4315-bd6b-8b71905634fd",
   "metadata": {},
   "source": [
    "## Outer Union with Null Masking\n",
    "\n",
    "We align all tenancy schedule tables using the assigned integration IDs and compute the outer union.\n",
    "\n",
    "- Only the columns with assigned IIDs are preserved.\n",
    "- If a table lacks a column present in the union, it is padded with a NaN. In the paper they call this a produced Null and it is a important part of why ALITE can cause a more bloated schema and is less business applicable according to this research. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95ebe5e1-c177-44e6-a280-4f7976c272ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_union(tables: Dict[str, pd.DataFrame], mapping):\n",
    "    all_iids = sorted({iid for mp in mapping.values() for iid in mp.values()})\n",
    "    frames, masks = [], []\n",
    "\n",
    "    for fname, df in tables.items():\n",
    "        df = df[[c for c in df.columns if c in mapping[fname]]].copy()\n",
    "        df.columns = [mapping[fname][c] for c in df.columns]\n",
    "\n",
    "        df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
    "        \n",
    "        orig_na = df.isna()        \n",
    "        for iid in all_iids:\n",
    "            if iid not in df.columns:\n",
    "                df[iid] = np.nan\n",
    "\n",
    "        true_na = pd.DataFrame(False, index=df.index, columns=all_iids)\n",
    "        true_na.loc[:, orig_na.columns] = orig_na\n",
    "        \n",
    "        frames.append(df[all_iids])\n",
    "        masks.append(true_na[all_iids])\n",
    "\n",
    "    return (pd.concat(frames, ignore_index=True),\n",
    "            all_iids,\n",
    "            pd.concat(masks, ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cacd5a1-cdae-48f5-ad58-2f3dd38a8cb5",
   "metadata": {},
   "source": [
    "## Full Disjunction FD\n",
    "\n",
    "This is the core of the ALITE framework, where we merge overlapping records and remove redundancy.\n",
    "\n",
    "Null labelling is done by labelling true missing values with placeholders.\n",
    "Produced nulls are treated as wildcards and empty strings during matching.\n",
    "\n",
    "During the iterative complementation steps tuples are merged if the overlap atleast >= MIN_OVERLAP non-null values. This helps unify partial records of the same entity while avoiding being over aggressive in mering based on more weak or coincidental matches.\n",
    "\n",
    "After merging, redundant tuples are dropped if they are completely dominated by more complete ones.\n",
    "\n",
    "Finally, placeholder labels and wildcards/empty strings are restored to NaN. This tries to yield a more compact and complete set of aligned tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c031110-8ea7-4e6b-a2a1-cf438c4c268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_union(df: pd.DataFrame, iids: list[str], mask: pd.DataFrame):\n",
    "    MIN_OVERLAP = 3\n",
    "    uid, df2 = 0, df.copy()\n",
    "    for iid in iids:\n",
    "        m = mask[iid]\n",
    "        df2.loc[m, iid] = [f\"NULL{uid+i}\" for i in range(int(m.sum()))]\n",
    "        uid += int(m.sum())\n",
    "\n",
    "    df2.replace({np.nan: \"\"}, inplace=True) \n",
    "    changed, rows = True, df2[iids].to_numpy().tolist()\n",
    "    while changed:\n",
    "        changed, out = False, []\n",
    "        for t in rows:\n",
    "            merged = False\n",
    "            for j, u in enumerate(out):\n",
    "                common = [k for k in range(len(iids)) if t[k] == u[k] and t[k] != \"\"]\n",
    "                if len(common) >= MIN_OVERLAP and all(t[k] == u[k] or t[k] == \"\" or u[k] == \"\" for k in range(len(iids))): \n",
    "                    out[j] = [t[k] if u[k]==\"\" else u[k] for k in range(len(iids))]\n",
    "                    changed = merged = True\n",
    "                    break\n",
    "            if not merged:\n",
    "                out.append(t)\n",
    "        rows = out\n",
    "\n",
    "    fd = pd.DataFrame(rows, columns=iids).replace({r\"^NULL\\d+$\": np.nan}, regex=True)\n",
    "    keep = []\n",
    "    \n",
    "    for i, r in fd.iterrows():\n",
    "        keep.append(not any(\n",
    "            i!=j and fd.loc[j].isna().sum() < r.isna().sum() and\n",
    "            all(pd.isna(r[c]) or fd.loc[j,c]==r[c] for c in iids)\n",
    "            for j in range(len(fd))))\n",
    "        \n",
    "    fd.replace(\"\", np.nan, inplace=True)\n",
    "    return fd[keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6695f-0682-45a9-b935-73780af6ba80",
   "metadata": {},
   "source": [
    "## Execution\n",
    "We now run the full pipeline:\n",
    "\n",
    "Load raw CSVs from disk (tenancy schedules in different formats). Assign IIDs via clustering then Outer-union tables. Run FD to complement and compact tuples. Finally export the standardised table to CSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "29b8b76c-3a2f-4c61-84fb-2b9c2a90b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_9092\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema width or no of IIDs:   89\n",
      "Final null % overall:    75.9%\n"
     ]
    }
   ],
   "source": [
    "tables = {p.name: pd.read_csv(p, dtype=str) for p in CFG[\"src_dir\"].glob(\"*.csv\")}\n",
    "\n",
    "iid_map = assign_iids(tables)\n",
    "ou, iids, m_mask = outer_union(tables, iid_map)\n",
    "fd = complement_union(ou, iids, m_mask)\n",
    "\n",
    "nulls = fd.isna().sum().sum()\n",
    "total = fd.size\n",
    "null_pct = nulls / total\n",
    "\n",
    "fd.to_csv(CFG[\"out_dir\"] / \"fd_result.csv\", index=False)\n",
    "\n",
    "print(\"Schema width or no of IIDs:  \", len(iids))\n",
    "print(\"Final null % overall:   \", f\"{null_pct:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e30d6899-f918-4ef2-abea-4df1045df1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tim'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6365c-ed1c-411f-a9d1-4edc4b00974a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 64-bit",
   "language": "python",
   "name": "jupyter_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
