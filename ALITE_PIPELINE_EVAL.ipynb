{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba09d2c-e381-46c3-8222-194d00849b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, itertools, json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances, silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Input is a folder filled with csv files of tenancy schedules\n",
    "\n",
    "CFG = {\n",
    "    \"src_dir\": Path(r\"C:\\Users\\tim\\Documents\\THESIS\\Data\\SCHEDULES\"),    \n",
    "    \"out_dir\": Path(r\"C:\\Users\\tim\\Documents\\THESIS\\Data\\processed\"),   \n",
    "}\n",
    "CFG[\"out_dir\"].mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba0cc74-7d61-4b7d-b661-ddf52f98bc6f",
   "metadata": {},
   "source": [
    "## Utlity helpers\n",
    "We have 2 helper functions here:\n",
    "\n",
    "clean: performs ASCII-based normalisation on column names for easier matching.\n",
    "\n",
    "embed_column: converts a single dataframe column into a vector embedding as described in ALITE using SBERT. (They used a research model here called TURL)  \n",
    "\n",
    "These embeddings are the foundation for clustering semantically similar columns across different tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eea2518-1658-4945-813f-6abda17abedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text).lower())\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\bsq\\s*m\\b\", \"sqm\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).replace(\"\\n\", \" \")\n",
    "    return text.strip()\n",
    "\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "def embed_column(col: pd.Series, col_name: str, sample: int = 50) -> np.ndarray:\n",
    "    non_null = col.dropna().astype(str)\n",
    "    if non_null.empty:\n",
    "        return np.zeros(embedder.get_sentence_embedding_dimension())\n",
    "\n",
    "    samples = non_null.sample(min(sample, len(non_null)), random_state=0).tolist()\n",
    "    inputs = [f\"{col_name}: {val}\" for val in samples]\n",
    "    vecs = embedder.encode(inputs, show_progress_bar=False)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f1fe4e-5e41-41fd-9fab-50844287c443",
   "metadata": {},
   "source": [
    "## Schema alignment through clustering columns\n",
    "\n",
    "This function assigns integration IDS to each column across all input tenancy schedule tables.\n",
    "\n",
    "STEPS:\n",
    "- Embed all columns using SBERT with the column name and value as context.\n",
    "  Example: Sample inputs to embedder:\n",
    "   - {Office+Restaurant: €194.801}\n",
    "   - {Office+Restaurant: €353.475}\n",
    "   - {Office+Restaurant: €359.398}\n",
    "- Compute cosine distance matrix.\n",
    "- Try to force that no columns from the same table are clustered by setting distances within table to 2.0\n",
    "- Hierarchical clustering is applied to group similar columns.\n",
    "- The number of clusters k is chosen using the silhouette score to maximise cluster quality/accuracy. The score indicates how well it merges and can be compared to see how well a column fit into other clusters.\n",
    "- Columns assigned to the same cluster are given the common integration ID (in final csv they are: iid_000 ,iid_001 etc).\n",
    "\n",
    "So in this part we try to group semantically similar columns without combining columns from the same schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e5bffd2-bd6a-4565-812c-ff2b2a81e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_iids(tables: Dict[str, pd.DataFrame]):\n",
    "    cols, vecs = [], []\n",
    "    for tname, df in tables.items():\n",
    "        for cname in df.columns:\n",
    "            v = embed_column(df[cname], cname)\n",
    "            if np.linalg.norm(v) > 1e-6:\n",
    "                cols.append((tname, cname))\n",
    "                vecs.append(v)\n",
    "\n",
    "    X = normalize(np.vstack(vecs))\n",
    "    D = pairwise_distances(X, metric=\"cosine\")\n",
    "\n",
    "    tbl = np.array([t for t, _ in cols])\n",
    "    D[tbl[:, None] == tbl[None, :]] = 2.0    \n",
    "\n",
    "    min_k = max(len(df.columns) for df in tables.values())\n",
    "    max_k = len(X) - 1\n",
    "    \n",
    "    if min_k == 1:\n",
    "        best_k, best_s = 1, -1\n",
    "    else:\n",
    "        best_k, best_s = min_k, -1\n",
    "        for k in range(min_k, max_k + 1):\n",
    "            labels = AgglomerativeClustering(n_clusters=k, metric=\"precomputed\", linkage=\"average\").fit_predict(D)\n",
    "            s = silhouette_score(X, labels, metric=\"cosine\")\n",
    "            if s > best_s:\n",
    "                best_k, best_s = k, s\n",
    "\n",
    "    labels = AgglomerativeClustering(n_clusters=best_k, metric=\"precomputed\", linkage=\"average\").fit_predict(D)\n",
    "    lab2iid = {lab: f\"iid_{i:03d}\" for i, lab in enumerate(np.unique(labels))}\n",
    "    mapping = {}\n",
    "    \n",
    "    for (t, c), lab in zip(cols, labels):\n",
    "        mapping.setdefault(t, {})[c] = lab2iid[lab]\n",
    "        \n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c2203-3929-4315-bd6b-8b71905634fd",
   "metadata": {},
   "source": [
    "## Outer union with null masking\n",
    "\n",
    "We align all tenancy schedule tables using the assigned integration IDs and compute the outer union.\n",
    "\n",
    "- Only the columns with assigned IIDs are preserved.\n",
    "- If a table lacks a column present in the union, it is padded with a NaN. In the paper they call this a produced Null and it is a important part of why ALITE can cause a more bloated schema and is less business applicable according to this research. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95ebe5e1-c177-44e6-a280-4f7976c272ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_union(tables: Dict[str, pd.DataFrame], mapping):\n",
    "    all_iids = sorted({iid for mp in mapping.values() for iid in mp.values()})\n",
    "    frames, masks = [], []\n",
    "\n",
    "    for fname, df in tables.items():\n",
    "        df = df[[c for c in df.columns if c in mapping[fname]]].copy()\n",
    "        df.columns = [mapping[fname][c] for c in df.columns]\n",
    "\n",
    "        df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
    "        \n",
    "        orig_na = df.isna()        \n",
    "        for iid in all_iids:\n",
    "            if iid not in df.columns:\n",
    "                df[iid] = np.nan\n",
    "\n",
    "        true_na = pd.DataFrame(False, index=df.index, columns=all_iids)\n",
    "        true_na.loc[:, orig_na.columns] = orig_na\n",
    "        \n",
    "        frames.append(df[all_iids])\n",
    "        masks.append(true_na[all_iids])\n",
    "\n",
    "    return (pd.concat(frames, ignore_index=True),\n",
    "            all_iids,\n",
    "            pd.concat(masks, ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cacd5a1-cdae-48f5-ad58-2f3dd38a8cb5",
   "metadata": {},
   "source": [
    "## Full Disjunction\n",
    "\n",
    "This is the core of the ALITE framework, where we merge overlapping records and remove redundancy.\n",
    "\n",
    "Null labelling is done by labelling true missing values with placeholders.\n",
    "Produced nulls are treated as wildcards and empty strings during matching.\n",
    "\n",
    "During the iterative complementation steps tuples are merged if the overlap atleast >= MIN_OVERLAP non-null values. This helps unify partial records of the same entity while avoiding being over aggressive in mering based on more weak or coincidental matches.\n",
    "\n",
    "After merging, redundant tuples are dropped if they are completely dominated by more complete ones.\n",
    "\n",
    "Finally, placeholder labels and wildcards/empty strings are restored to NaN. This tries to yield a more compact and complete set of aligned tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c031110-8ea7-4e6b-a2a1-cf438c4c268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_union(df: pd.DataFrame, iids: list[str], mask: pd.DataFrame):\n",
    "    MIN_OVERLAP = 3\n",
    "    uid, df2 = 0, df.copy()\n",
    "    for iid in iids:\n",
    "        m = mask[iid]\n",
    "        df2.loc[m, iid] = [f\"NULL{uid+i}\" for i in range(int(m.sum()))]\n",
    "        uid += int(m.sum())\n",
    "\n",
    "    df2.replace({np.nan: \"\"}, inplace=True) \n",
    "    changed, rows = True, df2[iids].to_numpy().tolist()\n",
    "    while changed:\n",
    "        changed, out = False, []\n",
    "        for t in rows:\n",
    "            merged = False\n",
    "            for j, u in enumerate(out):\n",
    "                common = [k for k in range(len(iids)) if t[k] == u[k] and t[k] != \"\"]\n",
    "                if len(common) >= MIN_OVERLAP and all(t[k] == u[k] or t[k] == \"\" or u[k] == \"\" for k in range(len(iids))): \n",
    "                    out[j] = [t[k] if u[k]==\"\" else u[k] for k in range(len(iids))]\n",
    "                    changed = merged = True\n",
    "                    break\n",
    "            if not merged:\n",
    "                out.append(t)\n",
    "        rows = out\n",
    "\n",
    "    fd = pd.DataFrame(rows, columns=iids).replace({r\"^NULL\\d+$\": np.nan}, regex=True)\n",
    "    keep = []\n",
    "    \n",
    "    for i, r in fd.iterrows():\n",
    "        keep.append(not any(\n",
    "            i!=j and fd.loc[j].isna().sum() < r.isna().sum() and\n",
    "            all(pd.isna(r[c]) or fd.loc[j,c]==r[c] for c in iids)\n",
    "            for j in range(len(fd))))\n",
    "        \n",
    "    fd.replace(\"\", np.nan, inplace=True)\n",
    "    return fd[keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6695f-0682-45a9-b935-73780af6ba80",
   "metadata": {},
   "source": [
    "## Execution\n",
    "We now run the full pipeline:\n",
    "\n",
    "Load raw CSVs from disk (tenancy schedules in different formats). Assign IIDs via clustering then Outer-union tables. Run FD to complement and compact tuples. Finally export the standardised table to CSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29b8b76c-3a2f-4c61-84fb-2b9c2a90b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n",
      "C:\\Users\\tim\\AppData\\Local\\Temp\\ipykernel_30220\\3212223805.py:9: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = (df.T.groupby(level=0, sort=False).agg(lambda col: col.bfill().infer_objects(copy=False).iloc[0]).T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of IIDs: 89\n",
      "Final null % overall:    75.86252478519498\n"
     ]
    }
   ],
   "source": [
    "tables = {p.name: pd.read_csv(p, dtype=str) for p in CFG[\"src_dir\"].glob(\"*.csv\")}\n",
    "\n",
    "iid_map = assign_iids(tables)\n",
    "ou, iids, m_mask = outer_union(tables, iid_map)\n",
    "fd = complement_union(ou, iids, m_mask)\n",
    "\n",
    "nulls = fd.isna().sum().sum()\n",
    "total = fd.size\n",
    "null_pct = nulls / total\n",
    "\n",
    "fd.to_csv(CFG[\"out_dir\"] / \"fd_result.csv\", index=False)\n",
    "\n",
    "print(\"Number of IIDs:\", len(iids))\n",
    "print(\"Final null % overall:   \", null_pct*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11cd6a-7a02-4fca-ab99-c3cc2d0e59cb",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This block evaluates the performance of the ALITE baseline configuration agains the manually labeled set of 20 semantically similair column sets.\n",
    "Metrics (precision/recall/f1) are calculated on a per-set basis and averaged out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1cce2e81-41d6-498a-8ca5-178f1bc56206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-set merge quality:\n",
      "                        FD_set  Precision   Recall       F1\n",
      "                       Address        1.0 0.166667 0.285714\n",
      "     Archive Leased Area (SQM)        1.0 0.266667 0.421053\n",
      "     Archive Leased Area (eur)        1.0 0.333333 0.500000\n",
      "              Break Date Lease        1.0 0.238095 0.384615\n",
      "            Expiray Date Lease        1.0 1.000000 1.000000\n",
      "               Extension terms        1.0 0.238095 0.384615\n",
      "                      Floor No        1.0 1.000000 1.000000\n",
      "                    Index date        1.0 1.000000 1.000000\n",
      "Notice period (# months/years)        1.0 1.000000 1.000000\n",
      "      Office Leased Area (SQM)        1.0 1.000000 1.000000\n",
      "      Office leased Area (eur)        1.0 0.266667 0.421053\n",
      "          Parking spaces (eur)        1.0 0.285714 0.444444\n",
      "        Parking spaces (units)        1.0 0.500000 0.666667\n",
      "           Property Identifier        1.0 0.100000 0.181818\n",
      "              Start Date Lease        1.0 0.285714 0.444444\n",
      "                   Tenant Name        1.0 1.000000 1.000000\n",
      "       Total Leased Area (SQM)        1.0 0.190476 0.320000\n",
      "       Total Leased Area (eur)        1.0 0.285714 0.444444\n",
      "       VAT Liability (boolean)        1.0 0.190476 0.320000\n",
      "                         WAULT        1.0 0.500000 0.666667\n",
      "Overall averaged):\n",
      "Precision: 1.0\n",
      "Recall   : 0.5375375375375375\n",
      "F1-score : 0.69921875\n"
     ]
    }
   ],
   "source": [
    "GT_PATH = Path(r\"C:\\Users\\tim\\Documents\\THESIS\\Data\\Ground_truth_FD.csv\")\n",
    "OUT_TSV = CFG[\"out_dir\"] / \"merge_eval.tsv\"\n",
    "\n",
    "gt = pd.read_csv(GT_PATH).fillna(\"NA\")\n",
    "formats = [col for col in gt.columns if col != \"FD\"]\n",
    "format2files = defaultdict(list)\n",
    "\n",
    "for fmt in formats:\n",
    "    for fname in tables:\n",
    "        if fmt.lower().replace(\"15\", \"\") in fname.lower():\n",
    "            format2files[fmt].append(fname)\n",
    "\n",
    "truth_sets = {}\n",
    "\n",
    "for _, row in gt.iterrows():\n",
    "    fd = row[\"FD\"]\n",
    "    matches = []\n",
    "\n",
    "    for fmt in formats:\n",
    "        val = row[fmt]\n",
    "        if val == \"NA\":\n",
    "            continue\n",
    "        val_clean = clean(val)\n",
    "        found = False\n",
    "        for fname in format2files[fmt]:\n",
    "            cleaned_cols = [clean(c) for c in tables[fname].columns]\n",
    "            for orig, cleaned in zip(tables[fname].columns, cleaned_cols):\n",
    "                if cleaned == val_clean:\n",
    "                    matches.append((fname, cleaned))\n",
    "                    found = True\n",
    "    truth_sets[fd] = matches\n",
    "\n",
    "pred_map = {\n",
    "    (fname, clean(col)): iid\n",
    "    for fname, mapping in iid_map.items()\n",
    "    for col, iid in mapping.items()\n",
    "}\n",
    "\n",
    "def all_pairs(lst):\n",
    "    return {tuple(sorted(p)) for p in itertools.combinations(lst, 2)}\n",
    "\n",
    "results = []\n",
    "tp_total = fp_total = fn_total = 0\n",
    "\n",
    "for fd, cols in truth_sets.items():\n",
    "    truth = all_pairs(cols)\n",
    "    present = [c for c in cols if c in pred_map]\n",
    "\n",
    "    predicted = {\n",
    "        tuple(sorted((a, b)))\n",
    "        for a, b in itertools.combinations(present, 2)\n",
    "        if pred_map[a] == pred_map[b]\n",
    "    }\n",
    "    tp = len(truth & predicted)\n",
    "    fp = len(predicted - truth)\n",
    "    fn = len(truth - predicted)\n",
    "    p = tp / (tp + fp) if (tp + fp) else 0\n",
    "    r = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1 = 2 * p * r / (p + r) if (p + r) else 0\n",
    "\n",
    "    results.append({\"FD_set\": fd, \"TP\": tp, \"FP\": fp, \"FN\": fn,\"Precision\":p,\"Recall\":r,\"F1\":f1})\n",
    "    tp_total += tp\n",
    "    fp_total += fp\n",
    "    fn_total += fn\n",
    "\n",
    "avg_p = tp_total / (tp_total + fp_total) if (tp_total + fp_total) else 0\n",
    "avg_r = tp_total / (tp_total + fn_total) if (tp_total + fn_total) else 0\n",
    "avg_f1 = 2 * micro_p * micro_r / (micro_p + micro_r) if (micro_p + micro_r) else 0\n",
    "\n",
    "df_result = pd.DataFrame(results).sort_values(\"FD_set\")\n",
    "print(\"Per-set merge quality:\")\n",
    "print(df_result[[\"FD_set\", \"Precision\", \"Recall\", \"F1\"]].to_string(index=False))\n",
    "\n",
    "print(f\"Overall averaged):\")\n",
    "print(f\"Precision: {avg_p}\")\n",
    "print(f\"Recall   : {avg_r}\")\n",
    "print(f\"F1-score : {avg_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8605c-45c3-42de-9402-742e04cf29d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
